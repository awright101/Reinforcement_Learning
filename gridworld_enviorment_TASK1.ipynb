{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld  Env For Testing & Experiementation Using Tabular Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our environment class \"Astroid Mining Gridworld\"\n",
    "\n",
    "- Our astroid mining agent rover, named SAMAR, aims to collect as many precious metal deposit, PMD, (+ve reward) which are randomly located across the astroid and navigating to the extraction point ( +ve reward) before it runs out of fuel ( -ve reward)\n",
    "\n",
    "\n",
    "\n",
    "- Inital fuel is parametizable\n",
    "\n",
    "\n",
    "\n",
    "- Fuel usage can be thought as a proxy for time spent, where each action SAMAR takes is one unit of fuel (-ve reward)\n",
    "\n",
    "\n",
    "\n",
    "- On the surface of the astroid there is randomly located 'sink holes' which cause SAMAR to get stuck and return a negative reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- It must reach an extraction point (+ve reward) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Termination conditions of the enviornment are reaching the extraction point or runnng out of fuel (large -ve reward) \n",
    "\n",
    "\n",
    "\n",
    "### State Space\n",
    "\n",
    "- The state space is given by the Length x Width definition of the grid,the edges are a impenetrable boundry meaning they cannot be occupied \n",
    "\n",
    "### Action Space\n",
    "\n",
    "- The action space is defined by movement of SAMAR: [left,right,up,down], as well as [retrive] which is done by picking up the precious metal (5 total actions)\n",
    "\n",
    "### Rewards\n",
    "\n",
    "- Each action costs 1 fuel unit, (reward = -1)\n",
    "\n",
    "- Landing on a sink hole costs up to 10 fuel units, (reward = [-1,-100])\n",
    "\n",
    "- Moving onto a PMD returns a reward of 20\n",
    "\n",
    "- Collecting a PMD returns a reward of +100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputError1(Exception):\n",
    "    \"\"\"Exception raised for errors in the input.\n",
    "\n",
    "    Attributes:\n",
    "        expression -- input expression in which the error occurred\n",
    "        message -- explanation of the error\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action, message='Action needs to be one of the following strings: up down left right retrive'):\n",
    "        self.action = action\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting GridWorld_Environment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile GridWorld_Environment.py\n",
    "import numpy as np\n",
    "class AstroidGrid():\n",
    "    \n",
    "    def __init__(self,length,width,ndeposits,nsinkholes,rand_locs,fuel):\n",
    "        self.randomseed1 = np.random.randint(1000)\n",
    "        self.randomseed2 = np.random.randint(1000)\n",
    "        #Setting up instance variables\n",
    "        self.length    = int(length)\n",
    "        self.width     = int(width)\n",
    "        self.ndeposits = int(ndeposits)\n",
    "        self.nsinkholes= int(nsinkholes)\n",
    "        self.rand_locs = rand_locs\n",
    "        \n",
    "        \n",
    "        #Position Vectors for exit and agent \n",
    "         \n",
    "        \n",
    "        self.exit_pos    = np.array([length-2,width-2])\n",
    "        \n",
    "        self.init_fuel   = fuel\n",
    "        #Inital Fuel\n",
    "        self.fuel_left   = fuel\n",
    "        #Cumulative Reward\n",
    "        self.reward_cum  = int(0)\n",
    "        \n",
    "        #Initalizing Environment \n",
    "        self.environment = None\n",
    "        self.samar_pos  = None\n",
    "        self.next_pos   = None\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        self.action_list = ['up','down','left','right','retrive']\n",
    "        \n",
    "        \n",
    "    def reset_env(self):\n",
    "        \n",
    "        #Initalizing Grid Array\n",
    "        \n",
    "        environment = np.zeros([self.length,self.width])\n",
    "        \n",
    "        #Setting Boundary Conditions\n",
    "        \n",
    "        environment[:,-1] = -1\n",
    "        environment[:,0]  = -1\n",
    "        environment[0,:]  = -1 \n",
    "        environment[-1,:] = -1\n",
    "        \n",
    "        #Setting Exit Position\n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "        \n",
    "        if self.rand_locs == False: \n",
    "            np.random.seed(self.randomseed1)\n",
    "            \n",
    "        for _ in range(self.ndeposits):\n",
    "            deposit_y,deposit_x = np.random.randint(1,self.length-1),np.random.randint(1,self.width-1)\n",
    "            environment[deposit_y,deposit_x] = 2\n",
    "        np.random.seed(seed=None)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.rand_locs == False: \n",
    "            np.random.seed(self.randomseed2)\n",
    "            \n",
    "        for _ in range(self.nsinkholes):\n",
    "            sinkhole_y,sinkhole_x = np.random.randint(1,self.length-1),np.random.randint(1,self.width-1)\n",
    "            environment[sinkhole_y,sinkhole_x] = 3\n",
    "        np.random.seed(seed=None)\n",
    "                \n",
    "\n",
    "            \n",
    "            \n",
    "        self.samar_pos = np.array([1,1]) # Orig. pos is hard coded\n",
    "        environment[self.samar_pos[0],self.samar_pos[1]]=0 #Setting value of agent square to 0 at start 1,1 will always be 0\n",
    "        environment[self.exit_pos[0],self.exit_pos[1]]  = 5 # Hard coded exit when \n",
    "        self.next_pos  = self.samar_pos \n",
    "        self.environment =environment\n",
    "        self.done = False #reset done\n",
    "        self.reward_cum  = int(0) #reset cumulative reward\n",
    "        self.fuel_left   = self.init_fuel #Resetting fuel (time) back to init fuel levels\n",
    "       \n",
    "    \n",
    "    def take_action(self,action):\n",
    "        \n",
    "        #Random action if action selected is movement \n",
    "        if action in ['up','down','left','right']:\n",
    "            \n",
    "            random_action = np.random.choice([0,1],p = [0.9,0.1])\n",
    "            \n",
    "            if random_action:\n",
    "                \n",
    "                action = np.random.choice(['up','down','left','right'])\n",
    "            \n",
    "      \n",
    "        if action == 'left':\n",
    "            self.next_pos = np.array([self.samar_pos[0],self.samar_pos[1] -1])\n",
    "        if action == 'right':\n",
    "            self.next_pos = np.array([self.samar_pos[0],self.samar_pos[1]  +1])\n",
    "        if action == 'down':\n",
    "            self.next_pos = np.array([self.samar_pos[0] +1,self.samar_pos[1]])\n",
    "        if action == 'up':  \n",
    "            self.next_pos = np.array([self.samar_pos[0] -1,self.samar_pos[1]])\n",
    "        if action == 'retrive':\n",
    "            self.next_pos = self.samar_pos\n",
    "            \n",
    "                                                            \n",
    "        #Reward responses for position resulting from  action\n",
    "            \n",
    "            #Next pos is a boundary\n",
    "        if self.environment[self.next_pos[0],self.next_pos[1]] == -1: \n",
    "            self.samar_pos = self.samar_pos #position doesnt changed as agent is at boundary\n",
    "            reward = -2\n",
    "            self.fuel_left += -1\n",
    "                \n",
    "            #Next pos is a free space    \n",
    "        if self.environment[self.next_pos[0],self.next_pos[1]] == 0: \n",
    "            self.samar_pos = self.next_pos\n",
    "            reward = -1\n",
    "            self.fuel_left += -1\n",
    "                \n",
    "            #Next pos is a PMD\n",
    "        if self.environment[self.next_pos[0],self.next_pos[1]] == 2:\n",
    "            self.samar_pos = self.next_pos\n",
    "            reward = 20\n",
    "            self.fuel_left += -1\n",
    "            \n",
    "            #Curr pos is a metal deposit \n",
    "        if self.environment[self.samar_pos[0],self.samar_pos[1]] == 2 and action=='retrive':\n",
    "            reward = 100\n",
    "            self.fuel_left += -1\n",
    "                \n",
    "                #Setting current location back to 0 as deposit has been collect\n",
    "            self.environment[self.samar_pos[0],self.samar_pos[1]] = 0\n",
    "            \n",
    "                  \n",
    "        if self.environment[self.samar_pos[0],self.samar_pos[1]] != 2 and action=='retrive':\n",
    "            reward = -10\n",
    "            self.fuel_left += -1\n",
    "                \n",
    "                \n",
    "        if self.environment[self.next_pos[0],self.next_pos[1]] == 3:\n",
    "            self.samar_pos = self.next_pos\n",
    "            reward = -1*np.random.randint(1,100)\n",
    "            self.fuel_left += -1\n",
    "                \n",
    "                \n",
    "        #TERMINATION OF ENVIRONMENT   \n",
    "        if self.environment[self.next_pos[0],self.next_pos[1]] == 5:\n",
    "            reward = 20\n",
    "            self.done = True #Termination condition reached \n",
    "            #print('Extraction Point Reached')\n",
    "        if self.fuel_left <1:\n",
    "            self.done = True\n",
    "            reward = -10\n",
    "            #print('Ran out of fuel')\n",
    "            \n",
    "        self.reward_cum += reward \n",
    "            \n",
    "        return reward,self.reward_cum,self.samar_pos,self.fuel_left,self.done\n",
    "            \n",
    "        \n",
    "        \n",
    "    def calc_obs(self):\n",
    "        \n",
    "        obs = self.environment[self.samar_pos[0]-1:self.samar_pos[0]+2,self.samar_pos[1]-1:self.samar_pos[1]+2]\n",
    "        \n",
    "        return obs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
